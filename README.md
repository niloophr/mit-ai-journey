# My Journey to MIT PhD in AI/ML

![Current Streak](https://img.shields.io/badge/Streak-15%20days%20unbroken-0066CC?style=for-the-badge&logo=github)

**Goal**: PhD at MIT EECS (CSAIL), Stanford, or CMU – Fall 2027/2028  
**Research Focus**: Parameter-Efficient Fine-Tuning (PEFT) of Large Language Models

From Islamic Azad University, Neyriz (Iran) → Cambridge, MA

![Python](https://img.shields.io/badge/Python-3.11-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.3-orange)
![HuggingFace](https://img.shields.io/badge/HuggingFace-PEFT-yellow)
![Progress](https://img.shields.io/badge/Progress-Phase%201-brightgreen)

### Current Phase (Nov 2025 – Feb 2026)
- Academic English (TOEFL ≥ 105)
- Math: Linear Algebra → Probability → Optimization
- Python/ML: Fast.ai → CS231n → HuggingFace PEFT

### Three Flagship PEFT Projects (2026)
| # | Project                              | Status    | Timeline      | Why it matters for MIT/Stanford |
|---|--------------------------------------|-----------|---------------|---------------------------------|
| 1 | LoRA from Scratch + Fine-tuning LLaMA-3-8B | Planned   | Mar–May 2026  | Pure implementation + full benchmark |
| 2 | QLoRA: 4-bit 70B Model Training on Single 24GB GPU | Planned   | Jun–Aug 2026  | State-of-the-art efficient training (2023–2025 hottest topic) |
| 3 | LongLoRA: Efficient 128k Context for LLaMA-3 | Planned   | Sep–Nov 2026  | Direct follow-up of 2024 LongLoRA paper – very few public impls |

All projects will include:
- Clean, fully documented code
- Full training/inference scripts
- Detailed comparison tables (memory, speed, performance)
- Gradio demo + Colab notebook

### Contact
- Email: niloophr.keshavarz@gmail.com
- LinkedIn: [Niloofar Keshavarz](https://www.linkedin.com/in/niloophr-keshavarz-6263b4313/)

Open to remote research collaboration with professors worldwide.  
Star ★ if you're rooting for this journey!

---
"From zero to CSAIL — one commit at a time." 
