![Road to CSAIL](https://images.unsplash.com/photo-1460925895917-afdab827c52f?ixlib=rb-4.0.3&auto=format&fit=crop&w=1350&q=80)
# My Journey to MIT PhD in AI/ML

**Goal**: PhD at MIT EECS (CSAIL), Stanford, or CMU – Fall 2027/2028  
**Research Focus**: Parameter-Efficient Fine-Tuning (PEFT) of Large Language Models

From Islamic Azad University, Neyriz (Iran) → Cambridge, MA

![Python](https://img.shields.io/badge/Python-3.11-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.3-orange)
![HuggingFace](https://img.shields.io/badge/HuggingFace-PEFT-yellow)
![Progress](https://img.shields.io/badge/Progress-Phase%201-brightgreen)

### Current Phase (Nov 2025 – Feb 2026)
- Academic English (TOEFL ≥ 105)
- Math: Linear Algebra → Probability → Optimization
- Python/ML: Fast.ai → CS231n → HuggingFace PEFT

### Three Flagship PEFT Projects (2026)
| # | Project                              | Status    | Timeline      | Why it matters for MIT/Stanford |
|---|--------------------------------------|-----------|---------------|---------------------------------|
| 1 | LoRA from Scratch + Fine-tuning LLaMA-3-8B | Planned   | Mar–May 2026  | Pure implementation + full benchmark |
| 2 | QLoRA: 4-bit 70B Model Training on Single 24GB GPU | Planned   | Jun–Aug 2026  | State-of-the-art efficient training (2023–2025 hottest topic) |
| 3 | LongLoRA: Efficient 128k Context for LLaMA-3 | Planned   | Sep–Nov 2026  | Direct follow-up of 2024 LongLoRA paper – very few public impls |

All projects will include:
- Clean, fully documented code
- Full training/inference scripts
- Detailed comparison tables (memory, speed, performance)
- Gradio demo + Colab notebook

### Contact
- Email: niloophr.keshavarz@gmail.com
- LinkedIn: linkedin.com/in/niloophr (create today if missing)

Open to remote research collaboration with professors worldwide.  
Star ★ if you're rooting for this journey!

---
"From zero to CSAIL — one commit at a time." 
